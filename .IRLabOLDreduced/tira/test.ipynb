{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thilo\\AppData\\Local\\Temp/ipykernel_24120/365715216.py:65: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  res_positive = es.search(index=\"final_boromir_index\", body=body_positive)\n",
      "C:\\Users\\thilo\\AppData\\Local\\Temp/ipykernel_24120/365715216.py:84: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  res_negative = es.search(index=\"final_boromir_index\", body=body_negative)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    topicID stance             pageID  rank      score    Method\n",
      "0         1    PRO  I2dda022878455420     1  85.991165  Boromir1\n",
      "1         1    PRO  I211441c0453def35     2  83.126816  Boromir1\n",
      "2         1    PRO  I9e60517c2520ebe5     3  79.457180  Boromir1\n",
      "3         1    PRO  Ic21db5313b5a4d9c     4  68.820786  Boromir1\n",
      "4         1    PRO  I72f92221cbde52e1     5  68.136475  Boromir1\n",
      "..      ...    ...                ...   ...        ...       ...\n",
      "995      50    CON  Ia545769f4e369b40     6  76.133160  Boromir1\n",
      "996      50    CON  Ie43ab05b64bdb708     7  72.860340  Boromir1\n",
      "997      50    CON  I03e2e1d48ab74152     8  61.380455  Boromir1\n",
      "998      50    CON  I7f81e8a4a867b336     9  53.233340  Boromir1\n",
      "999      50    CON  I09b98cea5f14f020    10  52.114150  Boromir1\n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from elasticsearch import Elasticsearch\n",
    "from os import listdir\n",
    "from numpy import RankWarning\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# #PARSE ARGUMENTS\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--input-dir\")\n",
    "# parser.add_argument(\"-o\", \"--output-dir\")\n",
    "# args = parser.parse_args()\n",
    "# args = vars(args)\n",
    "\n",
    "# #HANDLE INPUT\n",
    "# input_dir = args['input_dir']\n",
    "# output_dir = args['output_dir']\n",
    "\n",
    "# sleep(60)\n",
    "\n",
    "es = Elasticsearch(hosts=\"localhost\")\n",
    "\n",
    "while not es.ping():\n",
    "    sleep(10)\n",
    "    print(\"WAITING...\")\n",
    "    es = Elasticsearch(hosts=\"http://irlab_elastic_1:9200\", timeout=300)\n",
    "print(\"done waiting\")\n",
    "\n",
    "###############\n",
    "# make dictionary with all topics\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('topics.xml')\n",
    "# tree = ET.parse('topics_selected.xml')\n",
    "topics = tree.findall('topic')\n",
    "\n",
    "# Dictionary with all 50 topics\n",
    "topicsDic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    title = topic.find('title').text\n",
    "    number = topic.find('number').text\n",
    "    topicsDic[number] = title\n",
    "\n",
    "num_results = 10\n",
    "\n",
    "## first run: sentiment+OCR ###\n",
    "\n",
    "def search_refined_OCR_prototype(query, num_results):\n",
    "    body_positive = {\n",
    "        \"from\":0,\n",
    "        \"size\":num_results,\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match\": { \"document_text\":query}},\n",
    "                    {\"match\": {\"ocr_text\":{\"query\":query, \"boost\":5}}}  # ocr_text sollte einen natürlichen boost besitzen, da die texte viel kürzer sind. Vielleicht muss dieser auch abgeschwächt werden?\n",
    "                \n",
    "                ],\n",
    "                \"filter\":[\n",
    "                    {\"range\": {\"sentiment\": {\"gt\": 0}}}\n",
    "                ]\n",
    "            }   \n",
    "        }\n",
    "    }\n",
    "    res_positive = es.search(index=\"final_boromir_index\", body=body_positive)\n",
    "\n",
    "    body_negative = {\n",
    "        \"from\":0,\n",
    "        \"size\":num_results,\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match\": { \"document_text\":query}},\n",
    "                    {\"match\": {\"ocr_text\":{\"query\":query, \"boost\":5}}}  # ocr_text sollte einen natürlichen boost besitzen, da die texte viel kürzer sind. Vielleicht muss dieser auch abgeschwächt werden?\n",
    "                \n",
    "\n",
    "                ],\n",
    "                \"filter\":[\n",
    "                    {\"range\": {\"sentiment\": {\"lt\": 0}}}\n",
    "                ]            \n",
    "            }   \n",
    "        }\n",
    "    }\n",
    "    res_negative = es.search(index=\"final_boromir_index\", body=body_negative)\n",
    "\n",
    "    # get ID's from retrieved documents\n",
    "    resultPos = []\n",
    "    results_positive = res_positive.get('hits').get('hits')\n",
    "\n",
    "    for doc in results_positive:\n",
    "        id = doc.get('_id')\n",
    "        score = doc.get('_score')\n",
    "        resultPos.append([id, score])\n",
    "\n",
    "    resultNeg = []\n",
    "    results_negative = res_negative.get('hits').get('hits')\n",
    "\n",
    "    for doc in results_negative:\n",
    "        id = doc.get('_id')\n",
    "        score = doc.get('_score')\n",
    "        resultNeg.append([id, score])\n",
    "\n",
    "    return resultPos, resultNeg\n",
    "\n",
    "resultList = []\n",
    "for topic in topicsDic:\n",
    "    query = topicsDic.get(topic)\n",
    "    result_ids_pro, result_ids_con = search_refined_OCR_prototype(query, num_results)\n",
    "    i = 1\n",
    "    for each in result_ids_pro:\n",
    "        resultList.append([topic, \"PRO\", each[0], i, each[1], \"Boromir1\"])\n",
    "        i += 1\n",
    "    i = 1\n",
    "    for each in result_ids_con:\n",
    "        resultList.append([topic, \"CON\", each[0], i, each[1], \"Boromir1\"])\n",
    "        i += 1\n",
    "\n",
    "resultdf = pd.DataFrame(resultList, columns = ['topicID','stance','pageID','rank','score','Method'])\n",
    "with open(f'run.txt', 'a+') as f:\n",
    "    resultdf[['topicID','stance','pageID','rank','score','Method']].to_csv(f, sep=' ', header=False, index=False)\n",
    "\n",
    "print(resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thilo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n",
      "C:\\Users\\thilo\\AppData\\Local\\Temp/ipykernel_24120/4019254249.py:90: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  res_positive = es.search(index=\"final_boromir_index\", body=body_positive)\n",
      "C:\\Users\\thilo\\AppData\\Local\\Temp/ipykernel_24120/4019254249.py:134: DeprecationWarning: The 'body' parameter is deprecated for the 'search' API and will be removed in a future version. Instead use API parameters directly. See https://github.com/elastic/elasticsearch-py/issues/1698 for more information\n",
      "  res_negative = es.search(index=\"final_boromir_index\", body=body_negative)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done waiting\n",
      "    topicID stance             pageID  rank  score    Method\n",
      "0         1    PRO  I4592105915bb12fd     0    0.0  Boromir2\n",
      "1         1    PRO  I9bbcdf9b97e28232     0    0.0  Boromir2\n",
      "2         1    PRO  I912c93b9a9c4e7bb     0    0.0  Boromir2\n",
      "3         1    PRO  I6f276d3886624d52     0    0.0  Boromir2\n",
      "4         1    PRO  Idf7d5a8242c3c6ff     0    0.0  Boromir2\n",
      "..      ...    ...                ...   ...    ...       ...\n",
      "995      50    CON  I77b5c4ed96425a65     0    0.0  Boromir2\n",
      "996      50    CON  I011b22837ad2dead     0    0.0  Boromir2\n",
      "997      50    CON  I569956ec000d89b2     0    0.0  Boromir2\n",
      "998      50    CON  Ifa19e28004aaceba     0    0.0  Boromir2\n",
      "999      50    CON  Ib2fb69fd1ac42329     0    0.0  Boromir2\n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from elasticsearch import Elasticsearch\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# #PARSE ARGUMENTS\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--input-dir\")\n",
    "# parser.add_argument(\"-o\", \"--output-dir\")\n",
    "# args = parser.parse_args()\n",
    "# args = vars(args)\n",
    "\n",
    "# #HANDLE INPUT\n",
    "# input_dir = args['input_dir']\n",
    "# output_dir = args['output_dir']\n",
    "\n",
    "# sleep(60)\n",
    "\n",
    "es = Elasticsearch(hosts=\"localhost\")\n",
    "\n",
    "while not es.ping():\n",
    "    sleep(10)\n",
    "    print(\"WAITING...\")\n",
    "    es = Elasticsearch(hosts=\"localhost\", timeout=300)\n",
    "print(\"done waiting\")\n",
    "\n",
    "###############\n",
    "# make dictionary with all topics\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('topics.xml')\n",
    "# tree = ET.parse('topics_selected.xml')\n",
    "topics = tree.findall('topic')\n",
    "\n",
    "# Dictionary with all 50 topics\n",
    "topicsDic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    title = topic.find('title').text\n",
    "    number = topic.find('number').text\n",
    "    topicsDic[number] = title\n",
    "\n",
    "num_results = 10\n",
    "\n",
    "## second run: sentiment+ImageClustering ###\n",
    "\n",
    "def search_refined_Cluster(query, num_results):\n",
    "    body_positive = {\n",
    "        \"from\":0,\n",
    "        \"size\":num_results,\n",
    "        \"query\": {\n",
    "        \"function_score\": {\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match\": { \"document_text\":{\"query\":query}}},\n",
    "                ],\n",
    "\n",
    "                \"filter\":[\n",
    "                    {\"range\": {\"sentiment\": {\"gt\": 0}}}\n",
    "                ]\n",
    "            }   \n",
    "        },\n",
    "        #\"boost\": \"5\", \n",
    "        \"functions\": [\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"0\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"1\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"2\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"3\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"4\" } },\"weight\": 4.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"5\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"6\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"7\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"8\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"9\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"10\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"11\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"12\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"13\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"999\" } },\"weight\": 0.0}\n",
    "        ],      \n",
    "        \"max_boost\": 5.0,\n",
    "        \"score_mode\": \"max\",\n",
    "        \"boost_mode\": \"multiply\",\n",
    "        \"min_score\": 0.0\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res_positive = es.search(index=\"final_boromir_index\", body=body_positive)\n",
    "\n",
    "    body_negative = {\n",
    "        \"from\":0,\n",
    "        \"size\":num_results,\n",
    "        \"query\": {\n",
    "        \"function_score\": {\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match\": { \"document_text\":{\"query\":query}}},\n",
    "                ],\n",
    "\n",
    "                \"filter\":[\n",
    "                    {\"range\": {\"sentiment\": {\"lt\": 0}}}\n",
    "                ]\n",
    "            }   \n",
    "        },\n",
    "        #\"boost\": \"5\", \n",
    "        \"functions\": [\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"0\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"1\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"2\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"3\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"4\" } },\"weight\": 4.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"5\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"6\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"7\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"8\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"9\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"10\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"11\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"12\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"13\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"999\" } },\"weight\": 0.0}\n",
    "        ],      \n",
    "        \"max_boost\": 5.0,\n",
    "        \"score_mode\": \"max\",\n",
    "        \"boost_mode\": \"multiply\",\n",
    "        \"min_score\": 0.0\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    res_negative = es.search(index=\"final_boromir_index\", body=body_negative)\n",
    "\n",
    "    # get ID's from retrieved documents\n",
    "    result_positive_ids = []\n",
    "    results_positive = res_positive.get('hits').get('hits')\n",
    "\n",
    "    for doc in results_positive:\n",
    "        id = doc.get('_id')\n",
    "        result_positive_ids.append(id)\n",
    "\n",
    "    result_negative_ids = []\n",
    "    results_negative = res_negative.get('hits').get('hits')\n",
    "\n",
    "    for doc in results_negative:\n",
    "        id = doc.get('_id')\n",
    "        result_negative_ids.append(id)\n",
    "\n",
    "    return result_positive_ids, result_negative_ids\n",
    "\n",
    "resultList = []\n",
    "for topic in topicsDic:\n",
    "    query = topicsDic.get(topic)\n",
    "    result_ids_pro, result_ids_con = search_refined_Cluster(query, num_results)\n",
    "    for each in result_ids_pro:\n",
    "        resultList.append([topic, \"PRO\", each, 0, 0.0, \"Boromir2\"])\n",
    "    for each in result_ids_con:\n",
    "        resultList.append([topic, \"CON\", each, 0, 0.0, \"Boromir2\"])\n",
    "\n",
    "resultdf = pd.DataFrame(resultList, columns = ['topicID','stance','pageID','rank','score','Method'])\n",
    "with open(f'run.txt', 'a+') as f:\n",
    "    resultdf[['topicID','stance','pageID','rank','score','Method']].to_csv(f, sep=' ', header=False, index=False)\n",
    "\n",
    "print(resultdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from elasticsearch import Elasticsearch\n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "\n",
    "# #PARSE ARGUMENTS\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"-i\", \"--input-dir\")\n",
    "# parser.add_argument(\"-o\", \"--output-dir\")\n",
    "# args = parser.parse_args()\n",
    "# args = vars(args)\n",
    "\n",
    "# #HANDLE INPUT\n",
    "# input_dir = args['input_dir']\n",
    "# output_dir = args['output_dir']\n",
    "\n",
    "es = Elasticsearch(hosts=\"localhost\")\n",
    "\n",
    "while not es.ping():\n",
    "    sleep(10)\n",
    "    print(\"WAITING...\")\n",
    "    es = Elasticsearch(hosts=\"localhost\", timeout=300)\n",
    "print(\"done waiting\")\n",
    "\n",
    "###############\n",
    "# make dictionary with all topics\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('topics.xml')\n",
    "# tree = ET.parse('topics_selected.xml')\n",
    "topics = tree.findall('topic')\n",
    "\n",
    "# Dictionary with all 50 topics\n",
    "topicsDic = {}\n",
    "\n",
    "for topic in topics:\n",
    "    title = topic.find('title').text\n",
    "    number = topic.find('number').text\n",
    "    topicsDic[number] = title\n",
    "\n",
    "num_results = 10\n",
    "\n",
    "## third run: sentiment+OCR+ImageClustering###\n",
    "def search_refined_OCR_Clustering_prototype(query, num_results):\n",
    "    body_positive = {\n",
    "        \"from\":0,\n",
    "        \"size\":num_results,\n",
    "        \"query\": {\n",
    "        \"function_score\": {\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match\": { \"document_text\":{\"query\":query}}},\n",
    "                    {\"match\": {\"ocr_text\":{\"query\":query, \"boost\":5}}}\n",
    "                ],\n",
    "\n",
    "                \"filter\":[\n",
    "                    {\"range\": {\"sentiment\": {\"gt\": 0}}}\n",
    "                ]\n",
    "            }   \n",
    "        },\n",
    "        #\"boost\": \"5\", \n",
    "        \"functions\": [\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"0\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"1\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"2\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"3\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"4\" } },\"weight\": 4.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"5\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"6\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"7\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"8\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"9\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"10\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"11\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"12\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"13\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"999\" } },\"weight\": 0.0}\n",
    "        ],      \n",
    "        \"max_boost\": 5.0,\n",
    "        \"score_mode\": \"max\",\n",
    "        \"boost_mode\": \"multiply\",\n",
    "        \"min_score\": 0.0\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    res_positive = es.search(index=\"final_boromir_index\", body=body_positive)\n",
    "\n",
    "    body_negative = {\n",
    "        \"from\":0,\n",
    "        \"size\":num_results,\n",
    "        \"query\": {\n",
    "        \"function_score\": {\n",
    "        \"query\": {\n",
    "            \"bool\":{\n",
    "                \"should\":[\n",
    "                    {\"match\": { \"document_text\":{\"query\":query}}},\n",
    "                    {\"match\": {\"ocr_text\":{\"query\":query, \"boost\":5}}}\n",
    "                ],\n",
    "\n",
    "                \"filter\":[\n",
    "                    {\"range\": {\"sentiment\": {\"lt\": 0}}}\n",
    "                ]\n",
    "            }   \n",
    "        },\n",
    "        #\"boost\": \"5\", \n",
    "        \"functions\": [\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"0\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"1\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"2\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"3\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"4\" } },\"weight\": 4.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"5\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"6\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"7\" } },\"weight\": 3.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"8\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"9\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"10\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"11\" } },\"weight\": 5.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"12\" } },\"weight\": 1.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"13\" } },\"weight\": 2.0},\n",
    "            {\"filter\": { \"match\": { \"cluster_when_14_clusters_in_10dim\": \"999\" } },\"weight\": 0.0}\n",
    "        ],      \n",
    "        \"max_boost\": 5.0,\n",
    "        \"score_mode\": \"max\",\n",
    "        \"boost_mode\": \"multiply\",\n",
    "        \"min_score\": 0.0\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "    res_negative = es.search(index=\"final_boromir_index\", body=body_negative)\n",
    "\n",
    "    # get ID's from retrieved documents\n",
    "    result_positive_ids = []\n",
    "    results_positive = res_positive.get('hits').get('hits')\n",
    "\n",
    "    for doc in results_positive:\n",
    "        id = doc.get('_id')\n",
    "        result_positive_ids.append(id)\n",
    "\n",
    "    result_negative_ids = []\n",
    "    results_negative = res_negative.get('hits').get('hits')\n",
    "\n",
    "    for doc in results_negative:\n",
    "        id = doc.get('_id')\n",
    "        result_negative_ids.append(id)\n",
    "\n",
    "    return result_positive_ids, result_negative_ids\n",
    "\n",
    "resultList = []\n",
    "for topic in topicsDic:\n",
    "    query = topicsDic.get(topic)\n",
    "    result_ids_pro, result_ids_con = search_refined_OCR_Clustering_prototype(query, num_results)\n",
    "    for each in result_ids_pro:\n",
    "        resultList.append([topic, \"PRO\", each, 0, 0.0, \"Boromir3\"])\n",
    "    for each in result_ids_con:\n",
    "        resultList.append([topic, \"CON\", each, 0, 0.0, \"Boromir3\"])\n",
    "\n",
    "resultdf = pd.DataFrame(resultList, columns = ['topicID','stance','pageID','rank','score','Method'])\n",
    "with open(f'run.txt', 'a+') as f:\n",
    "    resultdf[['topicID','stance','pageID','rank','score','Method']].to_csv(f, sep=' ', header=False, index=False)\n",
    "\n",
    "print(resultdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_negative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24120/1257474666.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults_negative\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results_negative' is not defined"
     ]
    }
   ],
   "source": [
    "results_negative"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3898892d7e34557bb8499aff9aa0ccd3bf7bab375649613f01d0952879e4c360"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
