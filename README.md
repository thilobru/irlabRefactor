# IRLab Refactored Project

This project implements and evaluates an information retrieval system incorporating text preprocessing, OCR, sentiment analysis (dictionary-based and BERT), and image clustering to enhance search results, particularly for TREC-style topic queries with stance.

## Directory Structure
IRLab-Refactored/
├── config/                  # Configuration files
│   └── config.yaml          # Main config for paths, ES, parameters 
├── data/                    # Input data (or instructions)
│   ├── raw/                 # Original data files (TSVs, images, topics, qrels)
│   └── processed/           # Intermediate files generated by steps
├── docs/                    # Documentation
│   └── README.md            # This file
├── models/                  # Trained models
│   └── bertiment/           # Fine-tuned BERT sentiment model
│   └── bertiment_trained/   # Directory to save newly trained model
├── notebooks/               # Jupyter notebooks (optional, e.g., analysis)
│   └── run_evaluations.ipynb # Example notebook to trigger evaluations
├── results/                 # Output results
│   ├── evaluation/          # TSV files with evaluation scores per config
│   └── plots/               # Generated plots comparing results
├── src/                     # Main source code
│   ├── init.py
│   ├── data_loader.py       # Load raw data (TSV, XML topics, qrels)
│   ├── preprocessing.py     # Text preprocessing & OCR
│   ├── sentiment.py         # AFINN, VAD, BERT sentiment analysis
│   ├── clustering.py        # Image feature extraction & clustering
│   ├── elasticsearch_ops.py # Elasticsearch operations (create, index, search)
│   ├── evaluation.py        # Calculate metrics (P@k, MAP, NDCG@k) & run evaluations
│   ├── plotting.py          # Generate result plots
│   ├── bert_training.py     # (Optional) Script to train the BERT model
│   └── utils.py             # Utility functions (config loading, file ops)
├── main.py                  # Main command-line interface script 
└── requirements.txt         # Python dependencies 

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd IRLab-Refactored
    ```
2.  **Install System Dependencies:**
    * **Tesseract OCR Engine:** Required for OCR functionality[cite: 2]. Follow installation instructions for your OS: [https://github.com/tesseract-ocr/tesseract#installing-tesseract](https://github.com/tesseract-ocr/tesseract#installing-tesseract)
        * Ensure the `tesseract` command is in your system's PATH.
        * Install language packs needed (e.g., `eng`): `sudo apt-get install tesseract-ocr-eng` (Debian/Ubuntu) or download from the Tesseract repository.
3.  **Install Python Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *(See the updated `requirements.txt` below)*
4.  **Download NLTK Data:** The scripts attempt to download necessary NLTK data (`wordnet`, `stopwords`, `punkt`, `omw-1.4`) automatically upon first run. If this fails due to network restrictions, you can download them manually via `python -m nltk.downloader all`.
5.  **Obtain Data:**
    * Place raw data files (HTML titles TSV, images, topics XML, qrels file) in the `data/raw/` directory according to the paths specified in `config/config.yaml`[cite: 3].
    * Download lexicon files (AFINN, NRC-VAD) if using dictionary sentiment and place them where specified in `config.yaml`[cite: 3].
    * Download or obtain the pre-trained BERT sentiment model and place it in `models/bertiment/`[cite: 3], or train your own using the `--train-bert` flag (requires the IMDB dataset [cite: 3]).
6.  **Configure Elasticsearch:**
    * Ensure you have an Elasticsearch instance running.
    * Update the connection details (host, port, cloud ID, authentication) in `config/config.yaml` under the `elasticsearch` section[cite: 3].

## Configuration

The main configuration file is `config/config.yaml`[cite: 3]. Adjust the following sections as needed:

* `paths`: Define locations for raw data, processed files, models, and results.
* `elasticsearch`: Set connection details and the index name.
* `preprocessing`: Toggle lemmatization and stopword removal.
* `ocr`: Set Tesseract language and preprocessing options.
* `sentiment`: Paths to lexicons, BERT batch size.
* `clustering`: Feature extractor model, number of clusters, PCA options.
* `bert_training`: Parameters for training the BERT model (if used).
* `evaluation`: Define different named evaluation configurations specifying query fields, filters, boosts, metrics, etc.[cite: 3].
* `plotting`: Define plots to generate based on evaluation results[cite: 3].

## Usage (Command Line)

The primary way to run the workflow is via `main.py`[cite: 1]. Use command-line arguments to execute specific steps.

```bash
python main.py --config path/to/your/config.yaml [WORKFLOW_STEP_FLAGS]