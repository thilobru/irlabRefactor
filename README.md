# IRLab Refactored Project

This project implements and evaluates an information retrieval system incorporating text preprocessing, OCR, sentiment analysis (dictionary-based and BERT), and image clustering to enhance search results, particularly for TREC-style topic queries with stance.

## Directory Structure

```
IRLab-Refactored/
├── config/                  # Configuration files
│   └── config.yaml          # Main config for paths, ES, parameters
├── data/                    # Input data (or instructions)
│   ├── raw/                 # Original data files (TSVs, images, topics, qrels)
│   └── processed/           # Intermediate files generated by steps
├── docker-compose.yml       # Optional: Docker Compose for Elasticsearch
├── docs/                    # Documentation
│   └── README.md            # This file
├── models/                  # Trained models
│   └── bertiment/           # Fine-tuned BERT sentiment model
│   └── bertiment_trained/   # Directory to save newly trained model
├── notebooks/               # Jupyter notebooks (optional, e.g., analysis)
│   └── run_evaluations.ipynb # Example notebook to trigger evaluations
├── results/                 # Output results
│   ├── evaluation/          # TSV files with evaluation scores per config
│   └── plots/               # Generated plots comparing results
├── src/                     # Main source code
│   ├── __init__.py
│   ├── data_loader.py       # Load raw data (TSV, XML topics, qrels)
│   ├── preprocessing.py     # Text preprocessing & OCR
│   ├── sentiment.py         # AFINN, VAD, BERT sentiment analysis
│   ├── clustering.py        # Image feature extraction & clustering
│   ├── elasticsearch_ops.py # Elasticsearch operations (create, index, search)
│   ├── evaluation.py        # Calculate metrics (P@k, MAP, NDCG@k) & run evaluations
│   ├── plotting.py          # Generate result plots
│   ├── bert_training.py     # (Optional) Script to train the BERT model
│   └── utils.py             # Utility functions (config loading, file ops)
├── tests/                   # Unit tests
│   ├── fixtures/            # Test data (dummy files)
│   └── src/                 # Tests mirroring src structure
├── main.py                  # Main command-line interface script
├── pytest.ini               # Pytest configuration
└── requirements.txt         # Python dependencies
```

## Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd IRLab-Refactored
    ```
2.  **Install System Dependencies:**
    * **Docker:** Required if you want to run Elasticsearch in a container. Install Docker Desktop (Windows/Mac) or Docker Engine (Linux): [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)
    * **Tesseract OCR Engine:** Required for OCR functionality. Follow installation instructions for your OS: [https://github.com/tesseract-ocr/tesseract#installing-tesseract](https://github.com/tesseract-ocr/tesseract#installing-tesseract)
        * Ensure the `tesseract` command is in your system's PATH.
        * Install language packs needed (e.g., `eng`): `sudo apt-get install tesseract-ocr-eng` (Debian/Ubuntu) or download from the Tesseract repository.
3.  **Install Python Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
4.  **Download NLTK Data:** Run this Python script once to download necessary packages:
    ```python
    import nltk
    try:
        nltk.download('wordnet', quiet=True)
        nltk.download('stopwords', quiet=True)
        nltk.download('punkt', quiet=True)
        nltk.download('omw-1.4', quiet=True)
        print("NLTK data downloaded successfully (or already present).")
    except Exception as e:
        print(f"Error downloading NLTK data: {e}")
        print("Please ensure you have an internet connection or download manually.")
    ```
5.  **Obtain Data:**
    * Place raw data files (HTML titles TSV, images, topics XML, qrels file) in the `data/raw/` directory according to the paths specified in `config/config.yaml`.
    * Download lexicon files (AFINN, NRC-VAD) if using dictionary sentiment and place them where specified in `config.yaml`.
    * Download or obtain the pre-trained BERT sentiment model and place it in `models/bertiment/`, or train your own using the `--train-bert` flag (requires the IMDB dataset).
6.  **Configure Elasticsearch:**
    * **Option A: Use Docker (Recommended for Local Development)**
        * **Method 1: `docker run`**
            Open your terminal and run:
            ```bash
            docker run -d --name es01 -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" -e "xpack.security.enabled=false" elasticsearch:8.13.4
            ```
            * Replace `8.13.4` with your desired Elasticsearch version (ensure compatibility with your `elasticsearch` Python library version).
            * `-d`: Run in detached mode.
            * `--name es01`: Assign a name to the container.
            * `-p 9200:9200`: Map port 9200 on your host to port 9200 in the container (for HTTP REST API).
            * `-p 9300:9300`: Map port 9300 (for transport layer, optional for single node but good practice).
            * `-e "discovery.type=single-node"`: Crucial for starting a single node without bootstrap checks.
            * `-e "xpack.security.enabled=false"`: Disables security for easier local setup (DO NOT use in production).
            * *(Optional)* Add `-v es_data:/usr/share/elasticsearch/data` to persist data using a Docker volume named `es_data`.
        * **Method 2: `docker-compose`**
            Create a `docker-compose.yml` file in your project root with the following content:
            ```yaml
            version: '3.8'
            services:
              elasticsearch:
                image: elasticsearch:8.13.4 # Choose your version
                container_name: es01
                environment:
                  - discovery.type=single-node
                  - xpack.security.enabled=false
                  # - ES_JAVA_OPTS=-Xms512m -Xmx512m # Optional: Adjust heap size
                ports:
                  - "9200:9200"
                  - "9300:9300"
                # volumes: # Optional: Persist data
                #   - es_data:/usr/share/elasticsearch/data

            # volumes: # Optional: Define the volume
            #   es_data:
            #     driver: local
            ```
            Then run `docker-compose up -d` in your terminal from the project root.
        * **Connecting from Python:** Ensure your `config/config.yaml` points to the Docker instance:
            ```yaml
            elasticsearch:
              hosts: ["localhost"] # Or the IP if Docker runs elsewhere
              port: 9200
              # No cloud_id or auth needed if security is disabled
              index_name: "irlab_refactored_index" # Or your desired index name
              timeout: 60
            ```
    * **Option B: Use Existing Instance:** If you have another Elasticsearch instance (local, cloud), update the `hosts`, `port`, `cloud_id`, `api_key`, or `basic_auth` settings in `config/config.yaml` accordingly.

## Configuration

The main configuration file is `config/config.yaml`. Adjust the following sections as needed:

* `paths`: Define locations for raw data, processed files, models, and results.
* `elasticsearch`: Set connection details and the index name.
* `preprocessing`: Toggle lemmatization and stopword removal.
* `ocr`: Set Tesseract language and preprocessing options.
* `sentiment`: Paths to lexicons, BERT batch size.
* `clustering`: Feature extractor model, number of clusters, PCA options.
* `bert_training`: Parameters for training the BERT model (if used).
* `evaluation`: Define different named evaluation configurations specifying query fields, filters, boosts, metrics, etc..
* `plotting`: Define plots to generate based on evaluation results.

## Usage (Command Line)

The primary way to run the workflow is via `main.py`. Use command-line arguments to execute specific steps.

```bash
python main.py --config path/to/your/config.yaml [WORKFLOW_STEP_FLAGS]
```

**Available Workflow Steps:**

* `--preprocess-text`: Run text preprocessing on titles specified in config.
* `--run-ocr`: Run OCR on images in the specified folder.
* `--calculate-sentiment [dict|bert|all]`: Calculate sentiment using dictionary methods, BERT, or both.
* `--cluster-images`: Run image feature extraction and clustering.
* `--create-index`: **(Caution!)** Deletes the existing Elasticsearch index and creates a new one with the mapping defined in `elasticsearch_ops.py`. Requires Elasticsearch to be running and configured.
* `--index-data`: Load all processed data (titles, OCR, sentiment, clusters) and index it into Elasticsearch. Requires preceding steps that generate the data to have run successfully and Elasticsearch to be running.
* `--evaluate <config_name>`: Run a specific evaluation configuration defined in `config.yaml`. Requires data to be indexed and topics/qrels files to be present.
* `--evaluate-all`: Run all evaluation configurations defined in `config.yaml`.
* `--plot-results`: Generate plots based on the results saved in the evaluation output directory. Requires evaluations to have run.
* `--train-bert`: (Optional) Train the BERT sentiment model using the specified dataset and parameters.

**Example Workflow:**

```bash
# 0. Start Elasticsearch (e.g., using Docker)
# docker-compose up -d

# 1. Preprocess text and run OCR
python main.py --preprocess-text --run-ocr

# 2. Calculate both types of sentiment
python main.py --calculate-sentiment all

# 3. Cluster images
python main.py --cluster-images

# 4. Create index (first time or to reset)
python main.py --create-index

# 5. Index all processed data
python main.py --index-data

# 6. Run all defined evaluations
python main.py --evaluate-all

# 7. Generate plots
python main.py --plot-results
```

## Workflow Steps Details

1.  **Text Preprocessing (`preprocessing.py`)**: Tokenizes, lowercases, removes stopwords, and lemmatizes text data (e.g., HTML titles).
2.  **OCR (`preprocessing.py`)**: Extracts text from images using Tesseract, with optional image preprocessing (grayscale, thresholding, noise removal).
3.  **Sentiment Analysis (`sentiment.py`)**:
    * **Dictionary:** Calculates AFINN scores and average VAD (Valence, Arousal, Dominance) scores based on lexicons.
    * **BERT:** Uses a fine-tuned transformer model (like BERT) to classify text sentiment (e.g., Positive/Negative) and provide probabilities.
4.  **Image Clustering (`clustering.py`)**: Extracts image features (e.g., using VGG16), optionally applies PCA, and clusters images using K-Means. Saves cluster assignments and example images.
5.  **Indexing (`elasticsearch_ops.py`)**: Creates an Elasticsearch index with a specific mapping and indexes the combined processed data (titles, OCR, sentiment, clusters).
6.  **Evaluation (`evaluation.py`)**:
    * Runs searches against Elasticsearch based on topic queries and evaluation configurations from `config.yaml`.
    * Applies optional query preprocessing, sentiment filtering (based on topic stance), and cluster boosting.
    * Calculates standard IR metrics (Precision@k, MAP, NDCG@k) using provided relevance judgments (qrels).
    * Saves results per topic and averages to TSV files.
7.  **Plotting (`plotting.py`)**: Generates comparison plots (e.g., bar charts of metrics across different configurations) based on saved evaluation results.

## Testing

Unit tests are located in the `tests/` directory and can be run using `pytest`:

```bash
# Ensure you are in the project root directory
# Ensure tests/fixtures/ directory and files exist
pytest
